\href{https://github.com/rpng/open_vins/actions/workflows/build_ros1.yml}{\tt } \href{https://github.com/rpng/open_vins/actions/workflows/build_ros2.yml}{\tt } \href{https://github.com/rpng/open_vins/actions/workflows/build.yml}{\tt }

Welcome to the Open\+V\+I\+NS project! The Open\+V\+I\+NS project houses some core computer vision code along with a state-\/of-\/the art filter-\/based visual-\/inertial estimator. The core filter is an \href{https://en.wikipedia.org/wiki/Extended_Kalman_filter}{\tt Extended Kalman filter} which fuses inertial information with sparse visual feature tracks. These visual feature tracks are fused leveraging the \href{https://ieeexplore.ieee.org/document/4209642}{\tt Multi-\/\+State Constraint Kalman Filter (M\+S\+C\+KF)} sliding window formulation which allows for 3D features to update the state estimate without directly estimating the feature states in the filter. Inspired by graph-\/based optimization systems, the included filter has modularity allowing for convenient covariance management with a proper type-\/based state system. Please take a look at the feature list below for full details on what the system supports.


\begin{DoxyItemize}
\item Github project page -\/ \href{https://github.com/rpng/open_vins}{\tt https\+://github.\+com/rpng/open\+\_\+vins}
\item Documentation -\/ \href{https://docs.openvins.com/}{\tt https\+://docs.\+openvins.\+com/}
\item Getting started guide -\/ \href{https://docs.openvins.com/getting-started.html}{\tt https\+://docs.\+openvins.\+com/getting-\/started.\+html}
\item Publication reference -\/ \href{https://pgeneva.com/downloads/papers/Geneva2020ICRA.pdf}{\tt https\+://pgeneva.\+com/downloads/papers/\+Geneva2020\+I\+C\+R\+A.\+pdf}
\end{DoxyItemize}

\subsection*{News / Events}


\begin{DoxyItemize}
\item {\bfseries July 14, 2022} -\/ Improved feature extraction logic for $>$100hz tracking, some bug fixes and updated scripts. See v2.\+6.\+1 \href{https://github.com/rpng/open_vins/pull/259}{\tt PR\#259} and v2.\+6.\+2 \href{https://github.com/rpng/open_vins/pull/264}{\tt PR\#264}.
\item {\bfseries March 14, 2022} -\/ Initial dynamic initialization open sourcing, asynchronous subscription to inertial readings and publishing of odometry, support for lower frequency feature tracking. See v2.\+6 \href{https://github.com/rpng/open_vins/pull/232}{\tt PR\#232} for details.
\item {\bfseries December 13, 2021} -\/ New Y\+A\+ML configuration system, R\+O\+S2 support, Docker images, robust static initialization based on disparity, internal logging system to reduce verbosity, image transport publishers, dynamic number of features support, and other small fixes. See v2.\+5 \href{https://github.com/rpng/open_vins/pull/209}{\tt PR\#209} for details.
\item {\bfseries July 19, 2021} -\/ Camera classes, masking support, alignment utility, and other small fixes. See v2.\+4 \href{https://github.com/rpng/open_vins/pull/186}{\tt PR\#117} for details.
\item {\bfseries December 1, 2020} -\/ Released improved memory management, active feature pointcloud publishing, limiting number of features in update to bound compute, and other small fixes. See v2.\+3 \href{https://github.com/rpng/open_vins/pull/117}{\tt PR\#117} for details.
\item {\bfseries November 18, 2020} -\/ Released groundtruth generation utility package, \href{https://github.com/rpng/vicon2gt}{\tt vicon2gt} to enable creation of groundtruth trajectories in a motion capture room for evaulating V\+IO methods.
\item {\bfseries July 7, 2020} -\/ Released zero velocity update for vehicle applications and direct initialization when standing still. See \href{https://github.com/rpng/open_vins/pull/79}{\tt PR\#79} for details.
\item {\bfseries May 18, 2020} -\/ Released secondary pose graph example repository \href{https://github.com/rpng/ov_secondary}{\tt ov\+\_\+secondary} based on \href{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}{\tt V\+I\+N\+S-\/\+Fusion}. Open\+V\+I\+NS now publishes marginalized feature track, feature 3d position, and first camera intrinsics and extrinsics. See \href{https://github.com/rpng/open_vins/pull/66}{\tt PR\#66} for details and discussion.
\item {\bfseries April 3, 2020} -\/ Released \href{https://github.com/rpng/open_vins/releases/tag/v2.0}{\tt v2.\+0} update to the codebase with some key refactoring, ros-\/free building, improved dataset support, and single inverse depth feature representation. Please check out the \href{https://github.com/rpng/open_vins/releases/tag/v2.0}{\tt release page} for details.
\item {\bfseries January 21, 2020} -\/ Our paper has been accepted for presentation in \href{https://www.icra2020.org/}{\tt I\+C\+RA 2020}. We look forward to seeing everybody there! We have also added links to a few videos of the system running on different datasets.
\item {\bfseries October 23, 2019} -\/ Open\+V\+I\+NS placed first in the \href{http://rpg.ifi.uzh.ch/uzh-fpv.html}{\tt I\+R\+OS 2019 F\+PV Drone Racing V\+IO Competition}. We will be giving a short presentation at the \href{https://wp.nyu.edu/workshopiros2019mav/}{\tt workshop} at 12\+:45pm in Macau on November 8th.
\item {\bfseries October 1, 2019} -\/ We will be presenting at the \href{http://udel.edu/~ghuang/iros19-vins-workshop/index.html}{\tt Visual-\/\+Inertial Navigation\+: Challenges and Applications} workshop at \href{https://www.iros2019.org/}{\tt I\+R\+OS 2019}. The submitted workshop paper can be found at \href{http://udel.edu/~ghuang/iros19-vins-workshop/papers/06.pdf}{\tt this} link.
\item {\bfseries August 21, 2019} -\/ Open sourced \href{https://github.com/rpng/ov_maplab}{\tt ov\+\_\+maplab} for interfacing Open\+V\+I\+NS with the \href{https://github.com/ethz-asl/maplab}{\tt maplab} library.
\item {\bfseries August 15, 2019} -\/ Initial release of Open\+V\+I\+NS repository and documentation website!
\end{DoxyItemize}

\subsection*{Project Features}


\begin{DoxyItemize}
\item Sliding window visual-\/inertial M\+S\+C\+KF
\item Modular covariance type system
\item Comprehensive documentation and derivations
\item Extendable visual-\/inertial simulator
\begin{DoxyItemize}
\item On manifold S\+E(3) b-\/spline
\item Arbitrary number of cameras
\item Arbitrary sensor rate
\item Automatic feature generation
\end{DoxyItemize}
\item Five different feature representations
\begin{DoxyEnumerate}
\item Global X\+YZ
\item Global inverse depth
\item Anchored X\+YZ
\item Anchored inverse depth
\item Anchored M\+S\+C\+KF inverse depth
\item Anchored single inverse depth
\end{DoxyEnumerate}
\item Calibration of sensor intrinsics and extrinsics
\begin{DoxyItemize}
\item Camera to I\+MU transform
\item Camera to I\+MU time offset
\item Camera intrinsics
\end{DoxyItemize}
\item Environmental S\+L\+AM feature
\begin{DoxyItemize}
\item Open\+CV A\+R\+U\+CO tag S\+L\+AM features
\item Sparse feature S\+L\+AM features
\end{DoxyItemize}
\item Visual tracking support
\begin{DoxyItemize}
\item Monocular camera
\item Stereo camera (synchronized)
\item Binocular cameras (synchronized)
\item K\+LT or descriptor based
\item Masked tracking
\end{DoxyItemize}
\item Static and dynamic state initialization
\item Zero velocity detection and updates
\item Out of the box evaluation on Euroc\+Mav, T\+U\+M-\/\+VI, U\+Z\+H-\/\+F\+PV, K\+A\+I\+ST Urban and V\+IO datasets
\item Extensive evaluation suite (A\+TE, R\+PE, N\+E\+ES, R\+M\+SE, etc..)
\end{DoxyItemize}

\subsection*{Codebase Extensions}


\begin{DoxyItemize}
\item {\bfseries \href{https://github.com/rpng/vicon2gt}{\tt vicon2gt}} -\/ This utility was created to generate groundtruth trajectories using a motion capture system (e.\+g. Vicon or Opti\+Track) for use in evaluating visual-\/inertial estimation systems. Specifically we calculate the inertial I\+MU state (full 15 dof) at camera frequency rate and generate a groundtruth trajectory similar to those provided by the Euroc\+Mav datasets. Performs fusion of inertial and motion capture information and estimates all unknown spacial-\/temporal calibrations between the two sensors.
\item {\bfseries \href{https://github.com/rpng/ov_maplab}{\tt ov\+\_\+maplab}} -\/ This codebase contains the interface wrapper for exporting visual-\/inertial runs from \href{https://github.com/rpng/open_vins}{\tt Open\+V\+I\+NS} into the Vi\+Map structure taken by \href{https://github.com/ethz-asl/maplab}{\tt maplab}. The state estimates and raw images are appended to the Vi\+Map as Open\+V\+I\+NS runs through a dataset. After completion of the dataset, features are re-\/extract and triangulate with maplab\textquotesingle{}s feature system. This can be used to merge multi-\/session maps, or to perform a batch optimization after first running the data through Open\+V\+I\+NS. Some example have been provided along with a helper script to export trajectories into the standard groundtruth format.
\item {\bfseries \href{https://github.com/rpng/ov_secondary}{\tt ov\+\_\+secondary}} -\/ This is an example secondary thread which provides loop closure in a loosely coupled manner for \href{https://github.com/rpng/open_vins}{\tt Open\+V\+I\+NS}. This is a modification of the code originally developed by the H\+K\+U\+ST aerial robotics group and can be found in their \href{https://github.com/HKUST-Aerial-Robotics/VINS-Fusion}{\tt V\+I\+N\+S-\/\+Fusion} repository. Here we stress that this is a loosely coupled method, thus no information is returned to the estimator to improve the underlying Open\+V\+I\+NS odometry. This codebase has been modified in a few key areas including\+: exposing more loop closure parameters, subscribing to camera intrinsics, simplifying configuration such that only topics need to be supplied, and some tweaks to the loop closure detection to improve frequency.
\end{DoxyItemize}

\subsection*{Demo Videos}

\href{http://www.youtube.com/watch?v=KCX51GvYGss}{\tt } \href{http://www.youtube.com/watch?v=Lc7VQHngSuQ}{\tt } \href{http://www.youtube.com/watch?v=vaia7iPaRW8}{\tt } \href{http://www.youtube.com/watch?v=MCzTF9ye2zw}{\tt } \href{http://www.youtube.com/watch?v=eSQLWcNrx_I}{\tt } ~\newline


\href{http://www.youtube.com/watch?v=187AXuuGNNw}{\tt } \href{http://www.youtube.com/watch?v=oUoLlrFryk0}{\tt } \href{http://www.youtube.com/watch?v=ExPIGwORm4E}{\tt } \href{http://www.youtube.com/watch?v=lXHl-qgLGl8}{\tt }

\subsection*{Credit / Licensing}

This code was written by the \href{https://sites.udel.edu/robot/}{\tt Robot Perception and Navigation Group (R\+P\+NG)} at the University of Delaware. If you have any issues with the code please open an issue on our github page with relevant implementation details and references. For researchers that have leveraged or compared to this work, please cite the following\+:


\begin{DoxyCode}
@Conference\{Geneva2020ICRA,
  Title      = \{\{OpenVINS\}: A Research Platform \textcolor{keywordflow}{for} Visual-Inertial Estimation\},
  Author     = \{Patrick Geneva and Kevin Eckenhoff and Woosik Lee and Yulin Yang and Guoquan Huang\},
  Booktitle  = \{Proc. of the IEEE International Conference on Robotics and Automation\},
  Year       = \{2020\},
  Address    = \{Paris, France\},
  Url        = \{\(\backslash\)url\{https:\textcolor{comment}{//github.com/rpng/open\_vins\}\}}
\}
\end{DoxyCode}


The codebase and documentation is licensed under the \href{https://www.gnu.org/licenses/gpl-3.0.txt}{\tt G\+NU General Public License v3 (G\+P\+L-\/3)}. You must preserve the copyright and license notices in your derivative work and make available the complete source code with modifications under the same license (\href{https://choosealicense.com/licenses/gpl-3.0/}{\tt see this}; this is not legal advice). 