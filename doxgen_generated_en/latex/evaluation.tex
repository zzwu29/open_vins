The goal of our evaluation is to ensure fair comparison to other methods and our own. The actual metrics we use can be found on the \hyperlink{eval-metrics}{Filter Evaluation Metrics} page. Using our metrics we wish to provide insight into {\itshape why} our method does better and in what ways (as no method will outperform in all aspects). Since we are also interested in applying the systems to real robotic applications, the realtime performance is also a key metric we need to investigate. Timing of different system components is also key to removing bottlenecks and seeing where performance improvements or estimator approximations might help reduce complexity.

The key metrics we are interested in evaluating are the following\+:


\begin{DoxyItemize}
\item Absolute Trajectory Error (A\+TE)
\item Relative Pose Error (R\+PE)
\item Root Mean Squared Error (R\+M\+SE)
\item Normalized Estimation Error Squared (N\+E\+ES)
\item Estimator Component Timing
\item System Hardware Usage (memory and computation)
\end{DoxyItemize}\hypertarget{evaluation_evaluation-more}{}\section{System Evaluation Guides}\label{evaluation_evaluation-more}

\begin{DoxyItemize}
\item \hyperlink{eval-metrics}{Filter Evaluation Metrics} --- Definitions of different metrics for estimator accuracy.
\item \hyperlink{eval-error}{Filter Error Evaluation Methods} --- Error evaluation methods for evaluating system performance.
\item \hyperlink{eval-timing}{Filter Timing Analysis} --- Timing of estimator components and complexity. 
\end{DoxyItemize}\hypertarget{eval-metrics}{}\section{Filter Evaluation Metrics}\label{eval-metrics}
\hypertarget{eval-metrics_eval-ate}{}\subsection{Absolute Trajectory Error (\+A\+T\+E)}\label{eval-metrics_eval-ate}
The Absolute Trajectory Error (A\+TE) is given by the simple difference between the estimated trajectory and groundtruth after it has been aligned so that it has minimal error. First the \char`\"{}best\char`\"{} transform between the groundtruth and estimate is computed, afterwhich the error is computed at every timestep and then averaged. We recommend reading Zhang and Scaramuzza \cite{Zhang2018IROS} paper for details. For a given dataset with $N$ runs of the same algorithm with $K$ pose measurements, we can compute the following for an aligned estimated trajectory $\hat{\mathbf{x}}^+$\+:

\begin{align*} e_{ATE} &= \frac{1}{N} \sum_{i=1}^{N} \sqrt{ \frac{1}{K} \sum_{k=1}^{K} ||\mathbf{x}_{k,i} \boxminus \hat{\mathbf{x}}^+_{k,i}||^2_{2} } \end{align*}\hypertarget{eval-metrics_eval-rpe}{}\subsection{Relative Pose Error (\+R\+P\+E)}\label{eval-metrics_eval-rpe}
The Relative Pose Error (R\+PE) is calculated for segments of the dataset and allows for introspection of how localization solutions drift as the length of the trajectory increases. The other key advantage over A\+TE error is that it is less sensitive to jumps in estimation error due to sampling the trajectory over many smaller segments. This allows for a much fairer comparision of methods and is what we recommend all authors publish results for. We recommend reading Zhang and Scaramuzza \cite{Zhang2018IROS} paper for details. We first define a set of segment lengths $\mathcal{D} = [d_1,~d_2,\cdots,~d_V]$ which we compute the relative error for. We can define the relative error for a trajectory split into $D_i$ segments of $d_i$ length as follows\+:

\begin{align*} \tilde{\mathbf{x}}_{r} &= \mathbf{x}_{k} \boxminus \mathbf{x}_{k+d_i} \\ e_{rpe,d_i} &= \frac{1}{D_i} \sum_{k=1}^{D_i} ||\tilde{\mathbf{x}}_{r} \boxminus \hat{\tilde{\mathbf{x}}}_{r}||_{2} \end{align*}\hypertarget{eval-metrics_eval-rmse}{}\subsection{Root Mean Squared Error (\+R\+M\+S\+E)}\label{eval-metrics_eval-rmse}
When evaluating a system on a {\itshape single} dataset is the Root Mean Squared Error (R\+M\+SE) plots. This plots the R\+M\+SE at every timestep of the trajectory and thus can provide insight into timesteps where the estimation performance suffers. For a given dataset with $N$ runs of the same algorithm we can compute the following at each timestep $k$\+:

\begin{align*} e_{rmse,k} &= \sqrt{ \frac{1}{N} \sum_{i=1}^{N} ||\mathbf{x}_{k,i} \boxminus \hat{\mathbf{x}}_{k,i}||^2_{2} } \end{align*}\hypertarget{eval-metrics_eval-nees}{}\subsection{Normalized Estimation Error Squared (\+N\+E\+E\+S)}\label{eval-metrics_eval-nees}
Normalized Estimation Error Squared (N\+E\+ES) is a standard way to characterize if the estimator is being consistent or not. In general N\+E\+ES is just the normalized error which should be the degrees of freedoms of the state variables. Thus in the case of position and orientation we should get a N\+E\+ES of three at every timestep. To compute the average N\+E\+ES for a dataset with $N$ runs of the same algorithm we can compute the following at each timestep $k$\+:

\begin{align*} e_{nees,k} &= \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_{k,i} \boxminus \hat{\mathbf{x}}_{k,i})^\top \mathbf{P}^{-1}_{k,i} (\mathbf{x}_{k,i} \boxminus \hat{\mathbf{x}}_{k,i}) \end{align*}\hypertarget{eval-metrics_eval-singlerun}{}\subsection{Single Run Consistency}\label{eval-metrics_eval-singlerun}
When looking at a {\itshape single run} and wish to see if the system is consistent it is interesting to look a its error in respect to its estimated uncertainty. Specifically we plot the error and the estimator $3\sigma$ bound. This provides insight into if the estimator is becoming over confident at certain timesteps. Note this is for each component of the state, thus we need to plot x,y,z and orientation independently. We can directly compute the error at timestep $k$\+:

\begin{align*} \mathbf{e}_k &= \mathbf{x}_{k} \boxminus \hat{\mathbf{x}}_{k} \\ &\textrm{where} ~~\mathbf{e}_k\sim \mathcal N (0, \mathbf P) \end{align*} \hypertarget{eval-error}{}\section{Filter Error Evaluation Methods}\label{eval-error}


\begin{DoxyParagraph}{Installation Warning}
If you plan to use the included plotting from the cpp code, you will need to make sure that you have matplotlib and python 2.\+7 installed. We use the to \href{https://github.com/lava/matplotlib-cpp}{\tt matplotlib-\/cpp} to call this external library and generate the desired figures. Please see \hyperlink{gs-installing_gs-install-oveval}{Additional Evaluation Requirements} for more details on the exact install.
\end{DoxyParagraph}
\hypertarget{eval-error_eval-ov-collection}{}\subsection{Collection}\label{eval-error_eval-ov-collection}
The first step in any evaluation is to first collect the estimated trajectory of the proposed systems. Since we are interested in robotic application of our estimators we want to record the estimate at the current timestep (as compared to a \char`\"{}smoothed\char`\"{} output or one that includes loop-\/closures from future timesteps). Within the R\+OS framework, this means that we just need to publish the current estimate at the current timestep. We recommend using the following \hyperlink{classov__eval_1_1Recorder}{ov\+\_\+eval\+::\+Recorder} utility for recording the estimator output directly into a text file. Works with topics of type {\ttfamily Pose\+With\+Covariance\+Stamped}, {\ttfamily Pose\+Stamped}, {\ttfamily Transform\+Stamped}, and {\ttfamily Odometry}.


\begin{DoxyCode}
<\textcolor{keywordtype}{node} \textcolor{keyword}{name}=\textcolor{stringliteral}{"recorder\_estimate"} \textcolor{keyword}{pkg}=\textcolor{stringliteral}{"ov\_eval"} \textcolor{keyword}{type}=\textcolor{stringliteral}{"pose\_to\_file"} \textcolor{keyword}{output}=\textcolor{stringliteral}{"screen"}>
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"topic"}      \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/ov\_msckf/poseimu"} />
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"topic\_type"} \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"PoseWithCovarianceStamped"} />
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"output"}     \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/home/user/data/traj\_log.txt"} />
</\textcolor{keywordtype}{node}>
\end{DoxyCode}
\hypertarget{eval-error_eval-ov-transform}{}\subsection{Transformation}\label{eval-error_eval-ov-transform}
We now need to ensure both our estimated trajectory and groundtruth are in the correct formats for us to read in. We need things to be in the R\+PE text file format (i.\+e. time(s), px, py, pz, qx, qy, qz, qw). We have a nice helper script that will transform A\+SL / Eu\+RoC groundtruth files to the correct format. By default the Eu\+RoC groundtruth has the timestamp in nanoseconds and the quaternion is in an incorrect order (i.\+e. time(ns), px, py, pz, qw, qx, qy, qz). A user can either process all C\+SV files in a given folder, or just a specific one.


\begin{DoxyCode}
rosrun ov\_eval format\_convert folder/path/
rosrun ov\_eval format\_convert file.csv
\end{DoxyCode}


In addition we have a specific folder structure that is assumed. We store trajectories by first their algorithm name and then a folder for each dataset this algorithm was run on. The folder names of the datasets need to match the groundtruth trajectory files which should be in their own separate folder. Please see the example recorded datasets for how to structure your folders.


\begin{DoxyCode}
truth/
    dateset\_name\_1.txt
    dateset\_name\_2.txt
algorithms/
    open\_vins/
        dataset\_name\_1/
            run1.txt
            run2.txt
            run3.txt
        dataset\_name\_2/
            run1.txt
            run2.txt
            run3.txt
    okvis\_stereo/
        dataset\_name\_1/
            run1.txt
            run2.txt
            run3.txt
        dataset\_name\_2/
            run1.txt
            run2.txt
            run3.txt
    vins\_mono/
        dataset\_name\_1/
            run1.txt
            run2.txt
            run3.txt
        dataset\_name\_2/
            run1.txt
            run2.txt
            run3.txt
\end{DoxyCode}
\hypertarget{eval-error_eval-ov-plot}{}\subsection{Processing \& Plotting}\label{eval-error_eval-ov-plot}
Now that we have our data recorded and in the correct format we can now work on processing and plotting it. In the next few sections we detail how to do this for absolute trajectory error, relative pose error, normalized estimation error squared, and bounded root mean squared error plots. We will first process the data into a set of output text files which a user can then use to plot the results in their program or language of choice. The align mode of all the following commands can be of type {\ttfamily posyaw}, {\ttfamily posyawsingle}, {\ttfamily se3}, {\ttfamily se3single}, {\ttfamily sim3}, and {\ttfamily none}.\hypertarget{eval-error_eval-ov-plot-plot}{}\subsubsection{Script \char`\"{}plot\+\_\+trajectories\char`\"{}}\label{eval-error_eval-ov-plot-plot}
To plot the data we can us the following command which will plot a 2d xy and z-\/time position plots. It will use the filename as the name in the legend, so you can change that to change the legend or edit the code.


\begin{DoxyCode}
rosrun ov\_eval plot\_trajectories <align\_mode> <file\_gt.txt> ... <file\_est9.txt>
rosrun ov\_eval plot\_trajectories posyaw 1565371553\_estimate.txt truths/V1\_01\_easy.txt
\end{DoxyCode}


 \hypertarget{eval-error_eval-ov-plot-singlerun}{}\subsubsection{Script \char`\"{}error\+\_\+singlerun\char`\"{}}\label{eval-error_eval-ov-plot-singlerun}
The single run script will plot statistics and also 3 $\sigma$ bounds if available. One can use this to see consistency of the estimator or debug how the current run has gone. It also reports to console the average R\+M\+SE and R\+PE values for this run along with the number of samples. To change the R\+PE distances you will need to edit the code currently.


\begin{DoxyCode}
rosrun ov\_eval error\_singlerun <align\_mode> <file\_gt.txt> <file\_est.txt>
rosrun ov\_eval error\_singlerun posyaw 1565371553\_estimate.txt truths/V1\_01\_easy.txt
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
[ INFO] [1583422165.069376426]: [COMP]: 2813 poses in 1565371553\_estimate => length of 57.36 meters
[ INFO] [1583422165.091423722]: ======================================
[ INFO] [1583422165.091438299]: Absolute Trajectory Error
[ INFO] [1583422165.091445338]: ======================================
[ INFO] [1583422165.091453099]: rmse\_ori = 0.677 | rmse\_pos = 0.055
[ INFO] [1583422165.091459679]: mean\_ori = 0.606 | mean\_pos = 0.051
[ INFO] [1583422165.091466321]: min\_ori  = 0.044 | min\_pos  = 0.001
[ INFO] [1583422165.091474211]: max\_ori  = 1.856 | max\_pos  = 0.121
[ INFO] [1583422165.091481730]: std\_ori  = 0.302 | std\_pos  = 0.021
[ INFO] [1583422165.127869924]: ======================================
[ INFO] [1583422165.127891080]: Relative Pose Error
[ INFO] [1583422165.127898322]: ======================================
[ INFO] [1583422165.127908551]: seg 8 - median\_ori = 0.566 | median\_pos = 0.068 (2484 samples)
[ INFO] [1583422165.127919603]: seg 16 - median\_ori = 0.791 | median\_pos = 0.060 (2280 samples)
[ INFO] [1583422165.127927646]: seg 24 - median\_ori = 0.736 | median\_pos = 0.070 (2108 samples)
[ INFO] [1583422165.127934904]: seg 32 - median\_ori = 0.715 | median\_pos = 0.071 (1943 samples)
[ INFO] [1583422165.127942178]: seg 40 - median\_ori = 0.540 | median\_pos = 0.063 (1792 samples)
\end{DoxyCode}


\hypertarget{eval-error_eval-ov-plot-dataset}{}\subsubsection{Script \char`\"{}error\+\_\+dataset\char`\"{}}\label{eval-error_eval-ov-plot-dataset}
This dataset script will evaluate how a series of algorithms compare on a single dataset. Normally this is used if you just have single dataset you want to compare algorithms on, or compare a bunch variations of your algorithm to a simulated trajectory. In the console it will output the A\+TE 3D and 2D, along with the 3D R\+PE and 3D N\+E\+ES for each method after it performs alignment. To change the R\+PE distances you will need to edit the code currently.


\begin{DoxyCode}
rosrun ov\_eval error\_dataset <align\_mode> <file\_gt.txt> <folder\_algorithms>
rosrun ov\_eval error\_dataset posyaw truths/V1\_01\_easy.txt algorithms/
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
[ INFO] [1583422654.333642977]: ======================================
[ INFO] [1583422654.333915102]: [COMP]: processing mono\_ov\_slam algorithm
[ INFO] [1583422654.362655719]: [TRAJ]: q\_ESTtoGT = 0.000, 0.000, -0.129, 0.992 | p\_ESTinGT = 0.978, 2.185,
       0.932 | s = 1.00
....
[ INFO] [1583422654.996859432]: [TRAJ]: q\_ESTtoGT = 0.000, 0.000, -0.137, 0.991 | p\_ESTinGT = 0.928, 2.166,
       0.957 | s = 1.00
[ INFO] [1583422655.041009388]:     ATE: mean\_ori = 0.684 | mean\_pos = 0.057
[ INFO] [1583422655.041031730]:     ATE: std\_ori  = 0.14938 | std\_pos  = 0.01309
[ INFO] [1583422655.041039552]:     ATE 2D: mean\_ori = 0.552 | mean\_pos = 0.053
[ INFO] [1583422655.041046362]:     ATE 2D: std\_ori  = 0.17786 | std\_pos  = 0.01421
[ INFO] [1583422655.044187033]:     RPE: seg 7 - mean\_ori = 0.543 | mean\_pos = 0.065 (25160 samples)
[ INFO] [1583422655.047047771]:     RPE: seg 14 - mean\_ori = 0.593 | mean\_pos = 0.060 (23470 samples)
[ INFO] [1583422655.049672955]:     RPE: seg 21 - mean\_ori = 0.664 | mean\_pos = 0.081 (22050 samples)
[ INFO] [1583422655.052090494]:     RPE: seg 28 - mean\_ori = 0.732 | mean\_pos = 0.083 (20520 samples)
[ INFO] [1583422655.054294322]:     RPE: seg 35 - mean\_ori = 0.793 | mean\_pos = 0.090 (18960 samples)
[ INFO] [1583422655.055676035]:     RMSE: mean\_ori = 0.644 | mean\_pos = 0.056
[ INFO] [1583422655.056987984]:     RMSE 2D: mean\_ori = 0.516 | mean\_pos = 0.052
[ INFO] [1583422655.058269163]:     NEES: mean\_ori = 793.646 | mean\_pos = 13.095
[ INFO] [1583422656.182660653]: ======================================
[ INFO] [1583422656.183065588]: [COMP]: processing mono\_ov\_vio algorithm
[ INFO] [1583422656.209545279]: [TRAJ]: q\_ESTtoGT = 0.000, 0.000, -0.148, 0.989 | p\_ESTinGT = 0.931, 2.169,
       0.971 | s = 1.00
....
[ INFO] [1583422656.791523636]: [TRAJ]: q\_ESTtoGT = 0.000, 0.000, -0.149, 0.989 | p\_ESTinGT = 0.941, 2.163,
       0.974 | s = 1.00
[ INFO] [1583422656.835407991]:     ATE: mean\_ori = 0.639 | mean\_pos = 0.076
[ INFO] [1583422656.835433475]:     ATE: std\_ori  = 0.05800 | std\_pos  = 0.00430
[ INFO] [1583422656.835446222]:     ATE 2D: mean\_ori = 0.514 | mean\_pos = 0.070
[ INFO] [1583422656.835457020]:     ATE 2D: std\_ori  = 0.07102 | std\_pos  = 0.00492
[ INFO] [1583422656.838656567]:     RPE: seg 7 - mean\_ori = 0.614 | mean\_pos = 0.092 (25160 samples)
[ INFO] [1583422656.841540191]:     RPE: seg 14 - mean\_ori = 0.634 | mean\_pos = 0.092 (23470 samples)
[ INFO] [1583422656.844219466]:     RPE: seg 21 - mean\_ori = 0.632 | mean\_pos = 0.115 (22050 samples)
[ INFO] [1583422656.846646272]:     RPE: seg 28 - mean\_ori = 0.696 | mean\_pos = 0.119 (20520 samples)
[ INFO] [1583422656.848862913]:     RPE: seg 35 - mean\_ori = 0.663 | mean\_pos = 0.154 (18960 samples)
[ INFO] [1583422656.850321777]:     RMSE: mean\_ori = 0.600 | mean\_pos = 0.067
[ INFO] [1583422656.851673985]:     RMSE 2D: mean\_ori = 0.479 | mean\_pos = 0.060
[ INFO] [1583422656.853037942]:     NEES: mean\_ori = 625.447 | mean\_pos = 10.629
[ INFO] [1583422658.194763413]: ======================================
....
\end{DoxyCode}


\hypertarget{eval-error_eval-ov-plot-comparison}{}\subsubsection{Script \char`\"{}error\+\_\+comparison\char`\"{}}\label{eval-error_eval-ov-plot-comparison}
This compares all methods to each other on a series of datasets. For example, you run a bunch of methods on all the Euroc\+Mav datasets and then want to compare. This will do the R\+PE over all trajectories, and an A\+TE for each dataset. It will print the A\+TE and R\+PE for each method on each dataset in the console. Then following the \hyperlink{eval-metrics}{Filter Evaluation Metrics}, these are averaged over all the runs and datasets. Finally at the end it outputs a nice latex table which can be directly used in a paper. To change the R\+PE distances you will need to edit the code currently.


\begin{DoxyCode}
rosrun ov\_eval error\_comparison <align\_mode> <folder\_groundtruth> <folder\_algorithms>
rosrun ov\_eval error\_comparison posyaw truths/ algorithms/
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
[ INFO] [1583425216.054023187]: [COMP]: 2895 poses in V1\_01\_easy.txt => length of 58.35 meters
[ INFO] [1583425216.092355692]: [COMP]: 16702 poses in V1\_02\_medium.txt => length of 75.89 meters
[ INFO] [1583425216.133532429]: [COMP]: 20932 poses in V1\_03\_difficult.txt => length of 78.98 meters
[ INFO] [1583425216.179616651]: [COMP]: 22401 poses in V2\_01\_easy.txt => length of 36.50 meters
[ INFO] [1583425216.225299463]: [COMP]: 23091 poses in V2\_02\_medium.txt => length of 83.23 meters
[ INFO] [1583425216.225660364]: ======================================
[ INFO] [1583425223.560550101]: [COMP]: processing mono\_ov\_vio algorithm
[ INFO] [1583425223.560632706]: [COMP]: processing mono\_ov\_vio algorithm => V1\_01\_easy dataset
[ INFO] [1583425224.236769465]: [COMP]: processing mono\_ov\_vio algorithm => V1\_02\_medium dataset
[ INFO] [1583425224.855489521]: [COMP]: processing mono\_ov\_vio algorithm => V1\_03\_difficult dataset
[ INFO] [1583425225.659183593]: [COMP]: processing mono\_ov\_vio algorithm => V2\_01\_easy dataset
[ INFO] [1583425226.442217424]: [COMP]: processing mono\_ov\_vio algorithm => V2\_02\_medium dataset
[ INFO] [1583425227.366004330]: ======================================
....
[ INFO] [1583425261.724448413]: ============================================
[ INFO] [1583425261.724469372]: ATE LATEX TABLE
[ INFO] [1583425261.724481841]: ============================================
 & \(\backslash\)textbf\{V1\(\backslash\)\_01\(\backslash\)\_easy\} & \(\backslash\)textbf\{V1\(\backslash\)\_02\(\backslash\)\_medium\} & \(\backslash\)textbf\{V1\(\backslash\)\_03\(\backslash\)\_difficult\}
 & \(\backslash\)textbf\{V2\(\backslash\)\_01\(\backslash\)\_easy\} & \(\backslash\)textbf\{V2\(\backslash\)\_02\(\backslash\)\_medium\} & \(\backslash\)textbf\{Average\} \(\backslash\)\(\backslash\)\(\backslash\)hline
mono\(\backslash\)\_ov\(\backslash\)\_slam & 0.699 / 0.058 & 1.675 / 0.076 & 2.542 / 0.063 & 0.773 / 0.124 & 1.538 / 0.074 & 1.445 /
       0.079 \(\backslash\)\(\backslash\)
mono\(\backslash\)\_ov\(\backslash\)\_vio & 0.642 / 0.076 & 1.766 / 0.096 & 2.391 / 0.344 & 1.164 / 0.121 & 1.248 / 0.106 & 1.442 /
       0.148 \(\backslash\)\(\backslash\)
....
[ INFO] [1583425261.724647970]: ============================================
[ INFO] [1583425261.724655060]: ============================================
[ INFO] [1583425261.724661046]: RPE LATEX TABLE
[ INFO] [1583425261.724666910]: ============================================
 & \(\backslash\)textbf\{8m\} & \(\backslash\)textbf\{16m\} & \(\backslash\)textbf\{24m\} & \(\backslash\)textbf\{32m\} & \(\backslash\)textbf\{40m\} & \(\backslash\)textbf\{48m\} \(\backslash\)\(\backslash\)\(\backslash\)hline
mono\(\backslash\)\_ov\(\backslash\)\_slam & 0.661 / 0.074 & 0.802 / 0.086 & 0.979 / 0.097 & 1.061 / 0.105 & 1.145 / 0.120 & 1.289 /
       0.122 \(\backslash\)\(\backslash\)
mono\(\backslash\)\_ov\(\backslash\)\_vio & 0.826 / 0.094 & 1.039 / 0.106 & 1.215 / 0.111 & 1.283 / 0.132 & 1.342 / 0.151 & 1.425 /
       0.184 \(\backslash\)\(\backslash\)
....
[ INFO] [1583425262.514587296]: ============================================
\end{DoxyCode}


 \hypertarget{eval-timing}{}\section{Filter Timing Analysis}\label{eval-timing}


\begin{DoxyParagraph}{Installation Warning}
If you plan to use the included plotting from the cpp code, you will need to make sure that you have matplotlib and python 2.\+7 installed. We use the to \href{https://github.com/lava/matplotlib-cpp}{\tt matplotlib-\/cpp} to call this external library and generate the desired figures. Please see \hyperlink{gs-installing_gs-install-oveval}{Additional Evaluation Requirements} for more details on the exact install.
\end{DoxyParagraph}
\hypertarget{eval-timing_eval-ov-timing-collection}{}\subsection{Collection}\label{eval-timing_eval-ov-timing-collection}
To profile the different parts of the system we record the timing information from directly inside the \hyperlink{classov__msckf_1_1VioManager}{ov\+\_\+msckf\+::\+Vio\+Manager}. The file should be comma separated format, with the first column being the timing, and the last column being the total time (units are all in seconds). The middle columns should describe how much each component takes (whose names are extracted from the header of the csv file). You can use the bellow tools as long as you follow this format, and add or remove components as you see fit to the middle columns.

To evaluate the computational load ({\itshape not computation time}), we have a python script that leverages the \href{https://github.com/giampaolo/psutil}{\tt psutil} python package to record percent C\+PU and memory consumption. This can be included as an additional node in the launch file which only needs the node which you want the reported information of. This will poll the node for its percent memory, percent cpu, and total number of threads that it uses. This can be useful if you wish to compare different methods on the same platform, but doesn\textquotesingle{}t make sense to use this to compare the running of the same algorithm or different algorithms {\itshape across} different hardware devices.


\begin{DoxyCode}
<\textcolor{keywordtype}{node} \textcolor{keyword}{name}=\textcolor{stringliteral}{"recorder\_timing"} \textcolor{keyword}{pkg}=\textcolor{stringliteral}{"ov\_eval"} \textcolor{keyword}{type}=\textcolor{stringliteral}{"pid\_ros.py"} \textcolor{keyword}{output}=\textcolor{stringliteral}{"screen"}>
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"nodes"}   \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/run\_subscribe\_msckf"} />
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"output"}  \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/tmp/psutil\_log.txt"} />
</\textcolor{keywordtype}{node}>
\end{DoxyCode}


It is also important to note that if the estimator has multiple nodes, you can subscribe to them all by specifying their names as a comma separated string. For example to evaluate the computation needed for \href{https://github.com/HKUST-Aerial-Robotics/VINS-Mono/blob/master/vins_estimator/launch/euroc.launch}{\tt V\+I\+N\+S-\/\+Mono} multi-\/node system we can do\+:


\begin{DoxyCode}
<\textcolor{keywordtype}{node} \textcolor{keyword}{name}=\textcolor{stringliteral}{"recorder\_timing"} \textcolor{keyword}{pkg}=\textcolor{stringliteral}{"ov\_eval"} \textcolor{keyword}{type}=\textcolor{stringliteral}{"pid\_ros.py"} \textcolor{keyword}{output}=\textcolor{stringliteral}{"screen"}>
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"nodes"}   \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/feature\_tracker,/vins\_estimator,/pose\_graph"} />
    <\textcolor{keywordtype}{param} \textcolor{keyword}{name}=\textcolor{stringliteral}{"output"}  \textcolor{keyword}{type}=\textcolor{stringliteral}{"str"} \textcolor{keyword}{value}=\textcolor{stringliteral}{"/tmp/psutil\_log.txt"} />
</\textcolor{keywordtype}{node}>
\end{DoxyCode}
\hypertarget{eval-timing_eval-ov-timing-plot}{}\subsection{Processing \& Plotting}\label{eval-timing_eval-ov-timing-plot}
\hypertarget{eval-timing_eval-ov-timing-flame}{}\subsubsection{Script \char`\"{}timing\+\_\+flamegraph\char`\"{}}\label{eval-timing_eval-ov-timing-flame}
The flame graph script looks to recreate a \href{https://github.com/brendangregg/FlameGraph}{\tt Flame\+Graph} of the key components of the system. While we do not trace all functions, the key \char`\"{}top level\char`\"{} function times are recorded to file to allow for insight into what is taking the majority of the computation. The file should be comma separated format, with the first column being the timing, and the last column being the total time. The middle columns should describe how much each component takes (whose names are extracted from the header of the csv file).


\begin{DoxyCode}
rosrun ov\_eval timing\_flamegraph <file\_times.txt>
rosrun ov\_eval timing\_flamegraph timing\_mono\_ethV101.txt
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
[TIME]: loaded 2817 timestamps from file (7 categories)!!
mean\_time = 0.0037 | std = 0.0011 | 99th = 0.0063  | max = 0.0254 (tracking)
mean\_time = 0.0004 | std = 0.0001 | 99th = 0.0006  | max = 0.0014 ( propagation)
mean\_time = 0.0032 | std = 0.0022 | 99th = 0.0083  | max = 0.0309 (msckf update)
mean\_time = 0.0034 | std = 0.0013 | 99th = 0.0063  | max = 0.0099 (slam update)
mean\_time = 0.0012 | std = 0.0017 | 99th = 0.0052  | max = 0.0141 (slam delayed)
mean\_time = 0.0009 | std = 0.0003 | 99th = 0.0015  | max = 0.0027 (marginalization)
mean\_time = 0.0128 | std = 0.0035 | 99th = 0.0208  | max = 0.0403 (total)
\end{DoxyCode}


\hypertarget{eval-timing_eval-ov-timing-hist}{}\subsubsection{Script \char`\"{}timing\+\_\+histogram\char`\"{}}\label{eval-timing_eval-ov-timing-hist}
Generates a histogram plot of {\itshape binned} execution times for a specific class. This allows for inspection of the distribution of times as compared to just the mean of it. The file should be comma separated format, with the first column being the timing, and the last column being the total time. The middle columns should describe how much each component takes (whose names are extracted from the header of the csv file).


\begin{DoxyCode}
rosrun ov\_eval timing\_histogram <file\_times.txt> <num\_bins>
rosrun ov\_eval timing\_histogram timing\_mono\_ethV101.txt 75
\end{DoxyCode}


\hypertarget{eval-timing_eval-ov-timing-compare}{}\subsubsection{Script \char`\"{}timing\+\_\+comparison\char`\"{}}\label{eval-timing_eval-ov-timing-compare}
This script is use to compare the run-\/time of different runs of the algorithm. This take in the same file as the flame graph and is recorded in the \hyperlink{classov__msckf_1_1VioManager}{ov\+\_\+msckf\+::\+Vio\+Manager}. The file should be comma separated format, with the first column being the timing, and the last column being the total time. The middle columns should describe how much each component takes (whose names are extracted from the header of the csv file).


\begin{DoxyCode}
rosrun ov\_eval timing\_comparison <file\_times1.txt> ... <file\_timesN.txt>
rosrun ov\_eval timing\_comparison timing\_mono\_ethV101.txt timing\_stereo\_ethV101.txt
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
======================================
[TIME]: loading data for timing\_mono
[TIME]: loaded 2817 timestamps from file (7 categories)!!
mean\_time = 0.0037 | std = 0.0011 | 99th = 0.0063  | max = 0.0254 (tracking)
mean\_time = 0.0004 | std = 0.0001 | 99th = 0.0006  | max = 0.0014 (propagation)
mean\_time = 0.0032 | std = 0.0022 | 99th = 0.0083  | max = 0.0309 (msckf update)
mean\_time = 0.0034 | std = 0.0013 | 99th = 0.0063  | max = 0.0099 (slam update)
mean\_time = 0.0012 | std = 0.0017 | 99th = 0.0052  | max = 0.0141 (slam delayed)
mean\_time = 0.0009 | std = 0.0003 | 99th = 0.0015  | max = 0.0027 (marginalization)
mean\_time = 0.0128 | std = 0.0035 | 99th = 0.0208  | max = 0.0403 (total)
======================================
======================================
[TIME]: loading data for timing\_stereo
[TIME]: loaded 2817 timestamps from file (7 categories)!!
mean\_time = 0.0077 | std = 0.0020 | 99th = 0.0124  | max = 0.0219 (tracking)
mean\_time = 0.0004 | std = 0.0001 | 99th = 0.0007  | max = 0.0023 (propagation)
mean\_time = 0.0081 | std = 0.0047 | 99th = 0.0189  | max = 0.0900 (msckf update)
mean\_time = 0.0063 | std = 0.0023 | 99th = 0.0117  | max = 0.0151 (slam update)
mean\_time = 0.0020 | std = 0.0026 | 99th = 0.0079  | max = 0.0205 (slam delayed)
mean\_time = 0.0019 | std = 0.0005 | 99th = 0.0031  | max = 0.0052 (marginalization)
mean\_time = 0.0263 | std = 0.0063 | 99th = 0.0410  | max = 0.0979 (total)
======================================
\end{DoxyCode}


\hypertarget{eval-timing_eval-ov-timing-percentages}{}\subsubsection{Script \char`\"{}timing\+\_\+percentages\char`\"{}}\label{eval-timing_eval-ov-timing-percentages}
This utility allows for comparing the resources used by the algorithm on a specific platform. An example usage would be how the memory and cpu requirements increase as the state size grows or as more cameras are added. You will need to record the format using the {\ttfamily pid\+\_\+ros.\+py} node (see \hyperlink{eval-timing_eval-ov-timing-collection}{Collection} for details on how to use it). Remember that 100\% cpu usage means that it takes one cpu thread to support the system, while 200\% means it takes two cpu threads to support the system (see \href{https://superuser.com/a/457634/707974}{\tt this link} for an explanation). The folder structure needed is as follows\+:


\begin{DoxyCode}
psutil\_logs/
    ov\_mono/
        run1.txt
        run2.txt
        run3.txt
    ov\_stereo/
        run1.txt
        run2.txt
        run3.txt
\end{DoxyCode}



\begin{DoxyCode}
rosrun ov\_eval timing\_percentages <timings\_folder>
rosrun ov\_eval timing\_percentages psutil\_logs/
\end{DoxyCode}


Example output\+:


\begin{DoxyCode}
======================================
[COMP]: processing ov\_mono algorithm
    loaded 149 timestamps from file!!
    PREC: mean\_cpu = 83.858 +- 17.130
    PREC: mean\_mem = 0.266 +- 0.019
    THR: mean\_threads = 12.893 +- 0.924
======================================
======================================
[COMP]: processing ov\_stereo algorithm
    loaded 148 timestamps from file!!
    PREC: mean\_cpu = 111.007 +- 16.519
    PREC: mean\_mem = 5.139 +- 2.889
    THR: mean\_threads = 12.905 +- 0.943
======================================
\end{DoxyCode}


 